---
layout: post
title: "神经网络基础"
description: "基本神经网络-随机梯度下降"
category: tech
tags: [神经网络, 深度学习]
---
{% include JB/setup %}

本文学习资料参考：[Peter Yuan的博文][peteryuan].

## 神经网络
### S型神经元
S型函数定义为

<img src="http://www.forkosh.com/mathtex.cgi? \Large \sigma \left( z\right) \equiv \dfrac {1} {1+e^{-z}}">

### 梯度下降法
![Alt 梯度下降](/assets/themes/twitter/media/SGD.jpg "梯度下降")

//图片来自神经网络经典入门教程[Neural Networks and Deep Learning (Michael Nielsen)][nndl]
### 代价函数
二次代价函数

<img src="http://www.forkosh.com/mathtex.cgi? \Large C=\dfrac {1} {2n}\sum _{x}\left\| y\left( x\right) -a\right\| ^{2}">

交叉熵代价函数

<img src="http://www.forkosh.com/mathtex.cgi? \Large C=-\dfrac {1} {n}\sum _{x}\left[ y1na+\left( 1-y\right) \ln \left( 1-a\right) \right]">

## MNIST手写数字识别

### 结果
以二次函数（均方误差）作为目标函数：
{% highlight ruby %}
Epoch 0: 8061 / 10000
Epoch 1: 8341 / 10000
Epoch 2: 9266 / 10000
Epoch 3: 9305 / 10000
Epoch 4: 9390 / 10000
Epoch 5: 9410 / 10000
Epoch 6: 9410 / 10000
Epoch 7: 9423 / 10000
Epoch 8: 9405 / 10000
Epoch 9: 9422 / 10000
Epoch 10: 9454 / 10000
Epoch 11: 9438 / 10000
Epoch 12: 9458 / 10000
Epoch 13: 9484 / 10000
Epoch 14: 9464 / 10000
Epoch 15: 9478 / 10000
Epoch 16: 9494 / 10000
Epoch 17: 9485 / 10000
Epoch 18: 9495 / 10000
Epoch 19: 9494 / 10000
Epoch 20: 9498 / 10000
Epoch 21: 9487 / 10000
Epoch 22: 9513 / 10000
Epoch 23: 9515 / 10000
Epoch 24: 9515 / 10000
Epoch 25: 9525 / 10000
Epoch 26: 9505 / 10000
Epoch 27: 9494 / 10000
Epoch 28: 9491 / 10000
Epoch 29: 9512 / 10000
{% endhighlight %}

以交叉熵作为目标函数：
使用交叉熵的目标函数，学习网络可以更快速地从错误中学习（均方误差为目标函数时，当初始误差很大时，学习会变得非常缓慢。从均方误差函数特性可以看出，当误差趋向+∞/-∞时，均方误差函数都几乎为一条直线，斜率都非常小。所以，随机梯度下降学习方法会非常缓慢）。
{% highlight ruby %}
Epoch 0 training complete
Accuracy on evaluation data: 9141 / 10000

Epoch 1 training complete
Accuracy on evaluation data: 9244 / 10000

Epoch 2 training complete
Accuracy on evaluation data: 9335 / 10000

Epoch 3 training complete
Accuracy on evaluation data: 9335 / 10000

Epoch 4 training complete
Accuracy on evaluation data: 9414 / 10000

Epoch 5 training complete
Accuracy on evaluation data: 9428 / 10000

Epoch 6 training complete
Accuracy on evaluation data: 9442 / 10000

Epoch 7 training complete
Accuracy on evaluation data: 9463 / 10000

Epoch 8 training complete
Accuracy on evaluation data: 9456 / 10000

Epoch 9 training complete
Accuracy on evaluation data: 9490 / 10000

Epoch 10 training complete
Accuracy on evaluation data: 9480 / 10000

Epoch 11 training complete
Accuracy on evaluation data: 9477 / 10000

Epoch 12 training complete
Accuracy on evaluation data: 9485 / 10000

Epoch 13 training complete
Accuracy on evaluation data: 9506 / 10000

Epoch 14 training complete
Accuracy on evaluation data: 9498 / 10000

Epoch 15 training complete
Accuracy on evaluation data: 9494 / 10000

Epoch 16 training complete
Accuracy on evaluation data: 9529 / 10000

Epoch 17 training complete
Accuracy on evaluation data: 9524 / 10000

Epoch 18 training complete
Accuracy on evaluation data: 9539 / 10000

Epoch 19 training complete
Accuracy on evaluation data: 9508 / 10000

Epoch 20 training complete
Accuracy on evaluation data: 9481 / 10000

Epoch 21 training complete
Accuracy on evaluation data: 9524 / 10000

Epoch 22 training complete
Accuracy on evaluation data: 9500 / 10000

Epoch 23 training complete
Accuracy on evaluation data: 9495 / 10000

Epoch 24 training complete
Accuracy on evaluation data: 9501 / 10000

Epoch 25 training complete
Accuracy on evaluation data: 9516 / 10000

Epoch 26 training complete
Accuracy on evaluation data: 9513 / 10000

Epoch 27 training complete
Accuracy on evaluation data: 9529 / 10000

Epoch 28 training complete
Accuracy on evaluation data: 9504 / 10000

Epoch 29 training complete
Accuracy on evaluation data: 9505 / 10000

([], [9141, 9244, 9335, 9335, 9414, 9428, 9442, 9463, 9456, 9490, 9480, 9477, 94
85, 9506, 9498, 9494, 9529, 9524, 9539, 9508, 9481, 9524, 9500, 9495, 9501, 9516
, 9513, 9529, 9504, 9505], [], [])
{% endhighlight %}

一个具有对数似然代价的柔性最大值输出层，与一个具有交叉熵代价的S型输出层非常常相似。都可以解决学习缓慢问题。柔性最大值加上对数似
然的组合更加适用于那些需要将输出激活值解释为概率的场景（柔性最大值层的输出是一些和为1的正数的集合，柔性最大值层的输出可以被看做
是一个概率分布。）。
【完】

[peteryuan]: http://peteryuan.net/deep-learning-intro/
[nndl]: http://neuralnetworksanddeeplearning.com/
