---
layout: post
title: "神经网络中的基本概念"
description: "基本神经网络-随机梯度下降"
category: tech
tags: [神经网络, 深度学习]
---
{% include JB/setup %}

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
$$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$
\(x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\

本文学习资料参考：[Peter Yuan的博文][peteryuan].

## 神经网络
### 神经元
S型神经元，函数定义为：

<img src="http://www.forkosh.com/mathtex.cgi? \Large \sigma \left( z\right) \equiv \dfrac {1} {1+e^{-z}}">

tanh神经元，函数定义为：

<img src="http://www.forkosh.com/mathtex.cgi? \Large \tanh \left( z\right) \equiv \dfrac {e^{z}-e^{-z}} {e^{z}+e^{-z}}">

tanh仅仅是S型函数的按比例变化版本的版本，函数关系如下：

<img src="http://www.forkosh.com/mathtex.cgi? \Large \sigma \left( z\right) =\dfrac {1+\tanh \left( z / 2\right) } {2}">

这两个函数之间的一个差异就是tanh神经元的输出的值域是(1, 1)而非(0, 1)。这意味着如果你构建基于tanh神经元，你可能需要正规化最终的输出（取决于应用的细节，还有你的输入），跟sigmoid网络略微不同。

### 梯度下降法
梯度下降算法工作的方式就是重复计算梯度∇C，然后沿着相反的方向移动，沿着山谷“滚落”。我们可以想象它像这样：
![Alt 梯度下降](/assets/themes/twitter/media/SGD.jpg "梯度下降")

//图片来自神经网络经典入门教程[Neural Networks and Deep Learning (Michael Nielsen)][nndl]
### 代价函数
二次代价函数

<img src="http://www.forkosh.com/mathtex.cgi? \Large C=\dfrac {1} {2n}\sum _{x}\left\| y\left( x\right) -a\right\| ^{2}">

交叉熵代价函数

<img src="http://www.forkosh.com/mathtex.cgi? \Large C=-\dfrac {1} {n}\sum _{x}\left[ y \ln a+\left( 1-y\right) \ln \left( 1-a\right) \right]">

代价函数的两个假设：

第一个假设就是代价函数可以被写成一个在每个训练样本x上的代价函数Cx的均值：
<img src="http://www.forkosh.com/mathtex.cgi? \Large C=\dfrac {1} {n}\sum _{x}C_{x}">

第二个假设就是代价可以写成神经网络输出的函数。

### 反向传播

反向传播的4个方程式：

(BP1) <img src="http://www.forkosh.com/mathtex.cgi? \Large \delta ^{L}=\nabla _{a}C\odot \sigma '\left( z\right) ^{L}">

(BP2) <img src="http://www.forkosh.com/mathtex.cgi? \Large \delta ^{l}=\left( \left( w^{l+1}\right) ^{T}\delta ^{l+1}\right)\odot \sigma '\left( z\right) ^{l}">

(BP3) <img src="http://www.forkosh.com/mathtex.cgi? \Large \dfrac {\partial C} {\partial b_{j}^{l}}=\delta _{j}^{l}">

(BP4) <img src="http://www.forkosh.com/mathtex.cgi? \Large \frac{\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l">

### 过度拟合和规范化
规划化常用技术：

  (1) L1规范化
  
  (2) L2规范化
  
  (3) 弃权
  
  (4) 人为扩展训练数据
  
  
## MNIST手写数字识别

### 结果
以二次函数（均方误差）作为目标函数：
{% highlight ruby %}
Epoch 0: 8061 / 10000
Epoch 1: 8341 / 10000
Epoch 2: 9266 / 10000
Epoch 3: 9305 / 10000
Epoch 4: 9390 / 10000
Epoch 5: 9410 / 10000
Epoch 6: 9410 / 10000
Epoch 7: 9423 / 10000
Epoch 8: 9405 / 10000
Epoch 9: 9422 / 10000
Epoch 10: 9454 / 10000
Epoch 11: 9438 / 10000
Epoch 12: 9458 / 10000
Epoch 13: 9484 / 10000
Epoch 14: 9464 / 10000
Epoch 15: 9478 / 10000
Epoch 16: 9494 / 10000
Epoch 17: 9485 / 10000
Epoch 18: 9495 / 10000
Epoch 19: 9494 / 10000
Epoch 20: 9498 / 10000
Epoch 21: 9487 / 10000
Epoch 22: 9513 / 10000
Epoch 23: 9515 / 10000
Epoch 24: 9515 / 10000
Epoch 25: 9525 / 10000
Epoch 26: 9505 / 10000
Epoch 27: 9494 / 10000
Epoch 28: 9491 / 10000
Epoch 29: 9512 / 10000
{% endhighlight %}

以交叉熵作为目标函数：
使用交叉熵的目标函数，学习网络可以更快速地从错误中学习（均方误差为目标函数时，当初始误差很大时，学习会变得非常缓慢。从均方误差函数特性可以看出，当误差趋向+∞/-∞时，均方误差函数都几乎为一条直线，斜率都非常小。所以，随机梯度下降学习方法会非常缓慢）。
{% highlight ruby %}
Epoch 0 training complete
Accuracy on evaluation data: 9141 / 10000

Epoch 1 training complete
Accuracy on evaluation data: 9244 / 10000

Epoch 2 training complete
Accuracy on evaluation data: 9335 / 10000

Epoch 3 training complete
Accuracy on evaluation data: 9335 / 10000

Epoch 4 training complete
Accuracy on evaluation data: 9414 / 10000

Epoch 5 training complete
Accuracy on evaluation data: 9428 / 10000

Epoch 6 training complete
Accuracy on evaluation data: 9442 / 10000

Epoch 7 training complete
Accuracy on evaluation data: 9463 / 10000

Epoch 8 training complete
Accuracy on evaluation data: 9456 / 10000

Epoch 9 training complete
Accuracy on evaluation data: 9490 / 10000

Epoch 10 training complete
Accuracy on evaluation data: 9480 / 10000

Epoch 11 training complete
Accuracy on evaluation data: 9477 / 10000

Epoch 12 training complete
Accuracy on evaluation data: 9485 / 10000

Epoch 13 training complete
Accuracy on evaluation data: 9506 / 10000

Epoch 14 training complete
Accuracy on evaluation data: 9498 / 10000

Epoch 15 training complete
Accuracy on evaluation data: 9494 / 10000

Epoch 16 training complete
Accuracy on evaluation data: 9529 / 10000

Epoch 17 training complete
Accuracy on evaluation data: 9524 / 10000

Epoch 18 training complete
Accuracy on evaluation data: 9539 / 10000

Epoch 19 training complete
Accuracy on evaluation data: 9508 / 10000

Epoch 20 training complete
Accuracy on evaluation data: 9481 / 10000

Epoch 21 training complete
Accuracy on evaluation data: 9524 / 10000

Epoch 22 training complete
Accuracy on evaluation data: 9500 / 10000

Epoch 23 training complete
Accuracy on evaluation data: 9495 / 10000

Epoch 24 training complete
Accuracy on evaluation data: 9501 / 10000

Epoch 25 training complete
Accuracy on evaluation data: 9516 / 10000

Epoch 26 training complete
Accuracy on evaluation data: 9513 / 10000

Epoch 27 training complete
Accuracy on evaluation data: 9529 / 10000

Epoch 28 training complete
Accuracy on evaluation data: 9504 / 10000

Epoch 29 training complete
Accuracy on evaluation data: 9505 / 10000

([], [9141, 9244, 9335, 9335, 9414, 9428, 9442, 9463, 9456, 9490, 9480, 9477, 94
85, 9506, 9498, 9494, 9529, 9524, 9539, 9508, 9481, 9524, 9500, 9495, 9501, 9516
, 9513, 9529, 9504, 9505], [], [])
{% endhighlight %}

一个具有对数似然代价的柔性最大值输出层，与一个具有交叉熵代价的S型输出层非常常相似。都可以解决学习缓慢问题。柔性最大值加上对数似
然的组合更加适用于那些需要将输出激活值解释为概率的场景（柔性最大值层的输出是一些和为1的正数的集合，柔性最大值层的输出可以被看做
是一个概率分布。）。
【完】

[peteryuan]: http://peteryuan.net/deep-learning-intro/
[nndl]: http://neuralnetworksanddeeplearning.com/
